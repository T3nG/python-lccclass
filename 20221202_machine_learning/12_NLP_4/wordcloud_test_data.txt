中文自然語言處理比英文多了一個步驟，就是斷詞。

為什麼要多了斷詞這個步驟? 原因是英文的一個單字代表一個意思，比如 "traditional" 這一個單字的意思就是中文的 "傳統的"。英文只要一個單字 "traditional"，中文卻要用三個字來表示 "傳統的"。

把 "傳統的" 變成一個單字 (類似英文的一個單字)，這就是斷詞。

結巴斷詞

繁体中文的斷詞演算法目前大致有二套，一套是中研院開發的斷詞系統，另外一套是中國大陸開發的 jieba (結巴) 演算法。

中研院開發的東西大部份都可以直接跳過去，沒啥好期待的，只有 "非常爛" 一個詞、三個字來形容。相反的，大陸的 jieba 演算法，就非常容易且精準。

jieba 是模仿說話會結巴的人，把他們的斷點思維變成一個演算法。發明這種演算法的人，不但沒有瞧不起結巴的人，反而從中取得高招的思考方式，真的是超極聰明。

資料下載

開始進行中文斷詞之前，需下載二個資料，一個是繁体中文的字典，另一個是停用詞

字典下載 : https://github.com/APCLab/jieba-tw : 進入jieba目錄，點選 dict.txt，然後按右邊的 download

停用詞下載 : https://github.com/GoatWang/ithome_ironman/tree/master/day16_NLP_Chinese : 點選 stop.txt，按右上的 Raw 按鈕，將所有的資料全選並複製，然後開啟新的文字文件貼上，最後在專案的目錄下儲存成 stop.txt

安裝套件
pip install jieba wordcloud matplotlib
斷詞測試

底下的代碼，可以測試斷詞的效果。先把字典 dict.txt 及停用詞 stop.txt 載入。

斷詞的函數為 jieba.cut()，cut() 函數有二個模式，由 cut_all 參數控制。此參數如果為True 時，是精準全斷詞模式，會把所有能斷詞的方式都列出來，可以增加文與文之間的比較性，所以準確度會更高，但會更耗時。

如下的代碼，在全精準模式下會多出好幾種斷詞的方式。